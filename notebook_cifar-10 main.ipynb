{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "# For Cifar10 database -\n",
    "# You are given:\n",
    "# 1. Readme file has Template file to download cifar10 dataset (it has training and test dataset). You have to download dataset using this.\n",
    "# Your task is to:\n",
    "# 1. Predict correct class for every image in the test dataset.\n",
    "# Read Instructions carefully -\n",
    "# 1. Submit a csv file with only predictions for X test data. File should not have any headers and should only have one column i.e. predictions.\n",
    "# 2. Predictions should be class names and not numbers.\n",
    "# 3. Your score is based on number of accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we are going to use\n",
    "\n",
    "import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10.data_path = \"data/CIFAR-10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = cifar10.load_class_names()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial steps to get started with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know we just work on the input data in case of PCA, we just use the output data when we are going to perform predictions till then we just optimize the input data to make it fit for predictions in all cases, whether it is feature engineering or time optimization.\n",
    "\n",
    "This is what PCA means, when we do work on PCA this means we are working on the input data to reduce the features. \n",
    "Why do we reduce the features? It can get the bad predictions...\n",
    "\n",
    "We ususally do compromise some amount of precision when we compare the time. When the time is too huge and if the time can be greatly decreased without decreasing our precision much. We prefer to compromise the precision in the non-critical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_images = images_train.reshape(images_train.shape[0], -1) # We flattened the image, we did use reshape as it was in rgb\n",
    "reshaped_images_test = images_test.reshape(images_test.shape[0], -1) # Flattening the test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA() # Initializing PCA\n",
    "pca.fit(reshaped_images) # Fit the original data to PCA to get the eigen values and vectors to determine components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to the optimized input data with optimal k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above Initial steps, we made the data fit to get into PCA, as each pixel image can be considered an an feature. So we converted each pixel into individual column and applied PCA to it.\n",
    "\n",
    "So What basically we did is, We had those pixels as out features for each image. Now we have lacks and thousands of pixels. So it will take the enough time for the model to fit the data. So we tried to get rid of the features who had a very minor impact on our images. \n",
    "\n",
    "This is what eigen values show us. Eigen values show the magnitude of the variance in the eigen vectors. How much explaination each feature is giving to us is basically eigen value remains, and the plane at which that feature lies is basically revealed by the eigen vector.\n",
    "\n",
    "So, Below we will get the variance (Each features impact) of each feature and get it to a specified point. It is on us, we might like to keep 98% variance with us(means we would like keep 98% data with us). This is what k will be doing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "total = 0\n",
    "while(total < 0.95):\n",
    "    total += pca.explained_variance_ratio_[k]\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k, whiten = True) # Giving the component value to PCA\n",
    "transformed_data = pca.fit_transform(reshaped_images) # Transforming the train data\n",
    "test_images_transformed = pca.transform(reshaped_images_test) #Transforming the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below cell, we found the components, means the eigen vector for each feature. We then reshaped that into the shape in which our original dataset image was. \n",
    "\n",
    "Why did we do so?\n",
    "\n",
    "We did so to plot the same, in the form of images to analyse each feature. (If needed) Because the dimentions of image is required to visualize them. We can not visualize the image in the flattened way in the image form. Image must have pixeled dimentions to get visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pca.components_ # Eigen Vectors\n",
    "each_feature = np.reshape(features, (217, 32, 32, 3)) # Reshaping eigen vectors to visualize (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier() # Initializing the classifier\n",
    "clf.fit(transformed_data, cls_train) # Training the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_images_transformed) # Predicting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54      1055\n",
      "           1       0.56      0.50      0.53      1122\n",
      "           2       0.29      0.34      0.31       850\n",
      "           3       0.27      0.29      0.28       924\n",
      "           4       0.39      0.41      0.40       964\n",
      "           5       0.35      0.37      0.36       954\n",
      "           6       0.53      0.47      0.49      1129\n",
      "           7       0.40      0.49      0.44       811\n",
      "           8       0.59      0.51      0.55      1160\n",
      "           9       0.46      0.44      0.45      1031\n",
      "\n",
      "    accuracy                           0.44     10000\n",
      "   macro avg       0.44      0.43      0.44     10000\n",
      "weighted avg       0.45      0.44      0.44     10000\n",
      "\n",
      "[[554  43 114  42  53  33  17  48 101  50]\n",
      " [ 48 558  35  44  15  35  31  55  87 214]\n",
      " [ 53  12 290  88 132  86  96  61  17  15]\n",
      " [ 29  36  87 266  72 193  81  83  32  45]\n",
      " [ 34  14 150  75 392  74 116  74  23  12]\n",
      " [ 22  39  82 183  57 353  58 100  33  27]\n",
      " [ 24  27 118 119 145  87 525  42  18  24]\n",
      " [ 28  29  50  72  72  66  25 400  18  51]\n",
      " [163  66  47  45  38  36  23  44 594 104]\n",
      " [ 45 176  27  66  24  37  28  93  77 458]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicted, cls_test))\n",
    "print(confusion_matrix(predicted, cls_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane',\n",
    " 'automobile',\n",
    " 'bird',\n",
    " 'cat',\n",
    " 'deer',\n",
    " 'dog',\n",
    " 'frog',\n",
    " 'horse',\n",
    " 'ship',\n",
    " 'truck']\n",
    "cls_test = cls_test.astype(str)\n",
    "for i in range (len(classes)):\n",
    "    cls_test[cls_test == str(i)] = classes[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predictions.csv\", cls_test, fmt='%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
